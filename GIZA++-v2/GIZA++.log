Parameter 's' changed from '' to '/work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt.vcb'
Parameter 't' changed from '' to '/work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.src.vcb'
Parameter 'c' changed from '' to '/work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt_train_eval.src.snt'
Parameter 'p0' changed from '-1' to '0.98'
Parameter 'o' changed from '2023-09-21.060327.amaddha_umass_edu' to 'GIZA++'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 5  (number of iterations for Model 3)
model4iterations = 5  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-09-21.060327.amaddha_umass_edu.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 0  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 0  (1: do not write any files)
o = GIZA++  (output file prefix)
onlyaldumps = 0  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt_train_eval.src.snt  (training corpus file name)
d =   (dictionary file name)
s = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt.vcb  (source vocabulary file name)
t = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.src.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.2  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 64  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.98  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 5  (number of iterations for Model 3)
model4iterations = 5  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-09-21.060327.amaddha_umass_edu.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 0  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 0  (1: do not write any files)
o = GIZA++  (output file prefix)
onlyaldumps = 0  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt_train_eval.src.snt  (training corpus file name)
d =   (dictionary file name)
s = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt.vcb  (source vocabulary file name)
t = /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.src.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.2  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 64  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.98  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt.vcb
Reading vocabulary file from:/work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.src.vcb
Source vocabulary list has 9132 unique tokens 
Target vocabulary list has 9873 unique tokens 
Calculating vocabulary frequencies from corpus /work/pi_mccallum_umass_edu/amaddha_umass_edu/geoquery-span-alignment/data/smcalflow_cs/calflow.orgchart.event_create/source_domain_with_target_num128/train_eval.canonical.tgt_train_eval.src.snt
Reading more sentence pairs into memory ... 
{WARNING:(a)truncated sentence 578}{WARNING:(a)truncated sentence 12324}{WARNING:(a)truncated sentence 13392}{WARNING:(a)truncated sentence 15238}{WARNING:(a)truncated sentence 17246}{WARNING:(b)truncated sentence 23798}{WARNING:(b)truncated sentence 24629}{WARNING:(a)truncated sentence 25926}Corpus fits in memory, corpus has: 28181 sentence pairs.
 Train total # sentence pairs (weighted): 28181
Size of source portion of the training corpus: 799527 tokens
Size of the target portion of the training corpus: 330999 tokens 
In source portion of the training corpus, only 9128 unique tokens appeared
In target portion of the training corpus, only 9870 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 330999/(827708-28181)== 0.413994
ERROR: NO COOCURRENCE FILE GIVEN!
